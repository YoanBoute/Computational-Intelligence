{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Magic Square :\n",
    "\n",
    "2 | 7 | 6\n",
    "--+---+--\n",
    "9 | 5 | 1\n",
    "--+---+--\n",
    "4 | 3 | 8\n",
    "\n",
    "'''\n",
    "\n",
    "MAGIC = [2,7,6,\n",
    "         9,5,1,\n",
    "         4,3,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x','o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos : State) :\n",
    "    for r in range(3) :\n",
    "        for c in range(3) :\n",
    "            index = r*3 + c\n",
    "            if MAGIC[index] in pos.x :\n",
    "                print('X', end='')\n",
    "            elif MAGIC[index] in pos.o :\n",
    "                print('O', end='')\n",
    "            else :\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements) :\n",
    "    \"\"\"Check if positions of a player contain a full line\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements,3))\n",
    "\n",
    "def state_value(position : State) :\n",
    "    \"\"\"Returns 1 if x player wins, -1 if o player wins, 0 else\"\"\"\n",
    "    if win(position.x) :\n",
    "        return 1\n",
    "    elif win(position.o) :\n",
    "        return -1\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game() :\n",
    "    trajectory = list()\n",
    "    state = State(set(),set())\n",
    "    available = set(range(1, 9+1))\n",
    "    while available :\n",
    "        x = choice(list(available))\n",
    "        state.x.add(x)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(x)\n",
    "        if win(state.x) :\n",
    "            break\n",
    "        \n",
    "        if available == set() :\n",
    "            break\n",
    "        o = choice(list(available))\n",
    "        state.o.add(o)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(o)\n",
    "        if win(state.o) :\n",
    "            break\n",
    "    \n",
    "    return trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6631cae2ac4693aaa50f66c1a21627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_dictionary = defaultdict(float)\n",
    "epsilon = 0.001\n",
    "\n",
    "for steps in tqdm(range(100000)) :\n",
    "    trajectory = random_game()\n",
    "    final_reward = state_value(trajectory[-1])\n",
    "    for state in trajectory :\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "        value_dictionary[hashable_state] = value_dictionary[hashable_state] + epsilon*(final_reward - value_dictionary[hashable_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset({5}), frozenset()), 0.5161358325738735),\n",
       " ((frozenset({5}), frozenset({3})), 0.44638041049170357),\n",
       " ((frozenset({5}), frozenset({1})), 0.43392784060987294),\n",
       " ((frozenset({5}), frozenset({9})), 0.41540157043729864),\n",
       " ((frozenset({5}), frozenset({7})), 0.41084269218460745),\n",
       " ((frozenset({1, 2, 4, 7, 9}), frozenset({3, 5, 6, 8})), 0.40563411037998426),\n",
       " ((frozenset({3, 5, 6, 7, 9}), frozenset({1, 2, 4, 8})), 0.40503914952951375),\n",
       " ((frozenset({1, 2, 4, 5, 6}), frozenset({3, 7, 8, 9})), 0.40325069125445406),\n",
       " ((frozenset({1, 3, 5, 6, 9}), frozenset({2, 4, 7, 8})), 0.3954383734429431),\n",
       " ((frozenset({1, 5, 7, 8, 9}), frozenset({2, 3, 4, 6})), 0.39483320664959265)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(value_dictionary.items(), key=lambda e : e[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Compute all possible actions and resulting states for a player'''\n",
    "def next_possible_states(cur_state : State, player) :\n",
    "    possible_states = []\n",
    "    empty_places = set(range(1,9+1)) - cur_state.x - cur_state.o \n",
    "    for possible_play in empty_places :\n",
    "        if player == 'X' :\n",
    "            new_state = (frozenset(cur_state.x ^ set({possible_play})), frozenset(cur_state.o))\n",
    "        else :\n",
    "            new_state = (frozenset(cur_state.x), frozenset(cur_state.o ^ set({possible_play})))\n",
    "        possible_states.append(new_state)\n",
    "    \n",
    "    return possible_states\n",
    "\n",
    "'''Given a state of the game and the current player, returns the state resulting from the best play, based on the learned reward values'''\n",
    "def policy(cur_state : State, player, value_dictionary : dict) :\n",
    "    possible_states = next_possible_states(cur_state, player)\n",
    "    # If the player is X, the goal is to maximize the reward\n",
    "    if player == 'X' :\n",
    "        compare = lambda x,y : x > y\n",
    "        best_reward = -1 \n",
    "    # If the player is O, the goal is to minimize the reward\n",
    "    else :\n",
    "        compare = lambda x,y : x < y \n",
    "        best_reward = 1\n",
    "    best_state = None \n",
    "    for next in possible_states :\n",
    "        reward = value_dictionary[next]\n",
    "        # A reward of 0 means the state is not present in the dict\n",
    "        if reward == 0 :\n",
    "            continue\n",
    "        if compare(reward, best_reward) :\n",
    "            best_reward = reward \n",
    "            best_state = next \n",
    "    \n",
    "    # If no good move (if none of the next states are in the dict) can be found, play randomly\n",
    "    if best_state is None :\n",
    "        best_state = choice(possible_states)\n",
    "    \n",
    "    return State(set(best_state[0]), set(best_state[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Definition of a random agent, against which our policy will be tested'''\n",
    "def random_strat(state : State, player, value_dictionary = None) :\n",
    "    possible_states = next_possible_states(state, player)\n",
    "    rnd_state = choice(possible_states)\n",
    "    return State(set(rnd_state[0]), set(rnd_state[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(x={8, 1, 3}, o={9, 2, 4})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = State(set({3,8,1}),set({9,4}))\n",
    "player = 'O'\n",
    "policy(state, player, value_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the confidence interval of the mean of sample\n",
    "def compute_conf_interval(sample, conf_level) :\n",
    "    #print(sample)\n",
    "    mean = np.mean(sample)\n",
    "    sd = np.std(sample, ddof=1)\n",
    "    alpha = 1 - conf_level\n",
    "    n = len(sample)\n",
    "    if n <= 30 :\n",
    "        quantile = stats.t(df=n-1).ppf(1 - alpha/2)\n",
    "    else :\n",
    "        quantile = stats.norm().ppf(1 - alpha/2)\n",
    "    \n",
    "    delta = quantile*sd/np.sqrt(n)\n",
    "\n",
    "    return [mean - delta, mean + delta]\n",
    "\n",
    "# Checks if two intervals overlap or not\n",
    "def are_disjoint(itv1, itv2) -> bool :\n",
    "    if itv1[0] > itv2[1] :\n",
    "        return True\n",
    "    if itv2[0] > itv1[1] :\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Simulates a game between two strategies, and return 1 if the first strategy wins, -1 if the second strategy wins, or 0 if there is a draw'''\n",
    "def play_game(strat_1 : callable, strat_2 : callable, st1_is_first : bool, value_dictionary=None) :\n",
    "    state = State(set(),set())\n",
    "    available = set(range(1,9+1))\n",
    "    players = ['X','O']\n",
    "    players_strat = [strat_1, strat_2] if st1_is_first else [strat_2, strat_1] \n",
    "    ix_player = 0\n",
    "    while available :\n",
    "        strat = players_strat[ix_player]\n",
    "        player = players[ix_player]\n",
    "        state = strat(state, player, value_dictionary)\n",
    "        if win(state.x) :\n",
    "            return 1 if st1_is_first else -1\n",
    "        if win(state.o) :\n",
    "            return -1 if st1_is_first else 1\n",
    "        available = set(range(1,9+1)) - state.x - state.o\n",
    "        ix_player = 1 - ix_player\n",
    "\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game(policy, random_strat, False, value_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Performs multiple games (Until confidence intervals are separated) with a policy agent vs a random player. The policy agent is tested being 'X' and 'O' '''\n",
    "def test_policy(policy : callable, value_dictionary) :\n",
    "    policy_wins = list()\n",
    "    random_wins = list()\n",
    "    while True :\n",
    "        cnt_policy = 0\n",
    "        for _ in range(50) :\n",
    "            if play_game(policy, random_strat, st1_is_first=True, value_dictionary=value_dictionary) == 1 :\n",
    "                cnt_policy += 1\n",
    "        for _ in range(50) :\n",
    "            if play_game(policy, random_strat, st1_is_first=False, value_dictionary=value_dictionary) == 1 :\n",
    "                cnt_policy += 1\n",
    "        policy_wins.append(cnt_policy)\n",
    "        random_wins.append(100 - cnt_policy)\n",
    "        if len(policy_wins) == 1 :\n",
    "            continue\n",
    "        itv_policy = compute_conf_interval(policy_wins, 0.95)\n",
    "        itv_random = compute_conf_interval(random_wins, 0.95)\n",
    "        if are_disjoint(itv_policy, itv_random) :\n",
    "            return np.mean(itv_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy(policy, value_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "TODO :\n",
    "- Search for a better alternative to MonteCarlo\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = frozenset({1,2})\n",
    "set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
